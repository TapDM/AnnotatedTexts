<?xml version="1.0" encoding="UTF-8"?><html xmlns="http://www.w3.org/1999/xhtml" xmlns:oboInOwl="http://www.geneontology.org/formats/oboInOwl#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:obo="http://purl.obolibrary.org/obo/" xmlns:owl="http://www.w3.org/2002/07/owl#" xmlns:xsd="http://www.w3.org/2001/XMLSchema#" xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" xmlns:dublincorens="http://purl.org/dc/elements/1.1/"  version="XHTML+RDFa 1.0" ><body> <br /> Information assessment predicting protein-protein interactions <br />  <br />                  Abstract <br />                  Background: Identifying protein-protein interactions fundamental understanding                  molecular machinery cell. Proteome-wide studies protein-protein interactions                  significant value, high-throughput experimental technologies suffer high rates                  false positive false negative predictions. addition high-throughput experimental data,                  diverse types genomic data help predict protein-protein interactions, mRNA <br />                  expression, localization, essentiality, functional annotation. Evaluations information <br />                  contributions different evidences help establish parsimonious models                  comparable better prediction accuracy, obtain biological insights relationships <br />                  protein-protein interactions genomic information. <br />                  Results: assessment based genomic features used Bayesian network approach <br />                  predict protein-protein interactions genome-wide yeast. special case, does <br />                  missing information features, analysis shows larger <br />                  information contribution functional-classification expression correlations                  essentiality. case alternative models, logistic regression                  random forest, effective Bayesian networks predicting interactions. <br />                  Conclusions: restricted problem posed complete-information subset, identified <br />                  MIPS Gene Ontology   functional similarity datasets dominating information <br />                  contributors predicting protein-protein interactions framework proposed                  Jansen et al. Random forests based MIPS information highly accurate <br />                  classifications. particular subset complete information, adding genomic data does <br />                  little improving predictions. data discretizations used Bayesian <br />                  methods decreased classification performance. <br />  <br />  <br />  <br />  <br /> Background                                                                     catalyze large numbers <span id='am-1' about='obo:IMR_0000947' typeof='owl:Thing obo:IMR_0000001'><span id='am-2' property="rdfs:label" content="chemical" datatype="xsd:string"></span><span id='am-3' property="oboInOwl:hasOBONamespace" content="molecule_role" datatype="xsd:string"></span><span id='am-4' property="oboInOwl:id" content="IMR:0000947" datatype="xsd:string"></span>chemical</span> reactions, cru- <br /> Proteins transmit regulatory signals cell,                      cial stability numerous cellular structures. <br />  <br />                                                                                                                                         Page 1 11 <br />                                                                                                                   (page number citation purposes) <br />  BMC Bioinformatics 2004, 5:154                                              http://www.biomedcentral.com/1471-2105/5/154 <br />  <br />  <br />  <br /> Interactions proteins key cell functioning        (PIP) mRNA expression, Gene Ontology, identifying interactions crucial deciphering    MIPS functional coessentiality data. PIP fundamental molecular mechanisms cell. rel-       sub-network, different genomic features assumed evant genomic information exponentially increasing           independent prior. addition, method quantity complexity, silico predictions       involved discretizing raw data groups repre- <br /> protein-protein interactions possible        senting mRNA expression profiles (cell cycle challenging. number techniques devel-            Rosetta compendium data) principal compo- <br /> oped exploit combinations protein features           nent computational convenience. <br /> training data predict protein-protein interactions applied novel proteins. study motivated       current study focuses assessing contributions study Jansen et al. [1], proposed Bayesian           different types genomic data predicting pro- <br /> method use MIPS [2] complexes catalog gold            tein-protein interactions. help understand <br /> standard positives lists proteins separate subcel-    genomic features closest biological rela- <br /> lular compartments [3] gold standard negatives.          tionship protein-protein interactions various protein features considered method              construct better prediction model. prediction rules <br /> include time course mRNA expression fluctuations         involving relevant information lower pre- yeast cell cycle [4] Rosetta compendium [5],        diction accuracy, analysis insights biological function data Gene Ontology [6]         construct parsimonious models com- MIPS functional catalog, essentiality data [2],         parable better prediction accuracy. potential disad- <br /> high-throughput experimental interaction data [7-10].           vantage Bayesian network approach MIPS Gene Ontology functional annotations           data discretization obscure information contained used quantifying functional similarity          raw genomic data.  addition assessing proteins. MIPS functional catalog  biolog-        information content data sources, propose <br /> ical process annotation) thought hierarchi-      alternative non-Bayesian models fully utilize data <br /> cal tree functional classes  directed acyclic graph     discretization. methods, logistic <br /> (<span id='am-5' about='obo:IMR_0100027' typeof='obo:IMR_0100022 owl:Thing obo:IMR_0000947 obo:IMR_0001382 obo:IMR_0000001 obo:IMR_0001362'><span id='am-6' property="oboInOwl:hasExactSynonym" content="DAG" datatype="xsd:string"></span><span id='am-7' property="rdfs:label" content="Diglyceride" datatype="xsd:string"></span><span id='am-8' property="oboInOwl:hasExactSynonym" content="diacylglyceride" datatype="xsd:string"></span><span id='am-9' property="oboInOwl:hasExactSynonym" content="Diglyceride" datatype="xsd:string"></span><span id='am-10' property="rdfs:label" content="DAG" datatype="xsd:string"></span><span id='am-11' property="oboInOwl:hasOBONamespace" content="molecule_role" datatype="xsd:string"></span><span id='am-12' property="rdfs:label" content="diacylglycerol" datatype="xsd:string"></span><span id='am-13' property="rdfs:label" content="diacylglyceride" datatype="xsd:string"></span><span id='am-14' property="oboInOwl:hasDbXref" content="KEGG:C00165" datatype="xsd:string"></span><span id='am-15' property="oboInOwl:id" content="IMR:0100027" datatype="xsd:string"></span>DAG</span>) case . protein member       regression random forests, require prior member functional class,        knowledge, evaluate importance dif- <br /> protein describes &quot;subtree&quot; overall hierarchical       ferent genomic features context methods. <br /> tree classes  subgraph DAG case . <br /> Given proteins, compute intersection tree       Results discussions subtrees associated proteins.        accurately quantitatively assess information <br /> intersection tree computed complete list      contributions different genomic features, construct <br /> protein pairs  proteins pair      essence simplified problem functional classification), distribution inter-   elements original study.  look <br /> section trees obtained. &quot;functional similarity&quot;     subset data [1] comprising 18 million proteins defined frequency       protein pairs total approximately 8,000 gold stand- intersection tree proteins occurs dis-    ard positives 2.7 million gold standard negatives. tribution. Intuitively, intersection tree gives func-   subset  Additional File 1) contains 2,104 positives tional annotation proteins share.             172,409 negatives. subset, complete infor- <br /> ubiquitous shared functional annotation  larger     mation feature quantitatively functional similarity frequency; specific       assess relative contributions different features shared functional annotation  smaller         set. data set downloaded http:// <br /> functional similarity frequency. essentiality data rep-     bioinformatics.med.yale.edu/PPI. doing  resents categorical variable denotes zero,       features stronger influence proteins protein pair essential.       overall prediction. true larger <br /> supplementary online material [1]http://www.science          problem  number caveats mag.org/cgi/data/302/5644/449/DC1/1 provides               mind, features details quantification variables.      present subset strongest Bayesian method predicts protein-protein interactions           set 18 million protein pairs. <br /> genome-wide probabilistic integration genomic fea- <br /> tures weakly associated interactions (mRNA        Alternative models <br /> expression, essentiality localization). model        construct models predicting protein-protein <br /> used separate predictions probabilistic interac-     interactions  given gold standards, basically <br /> tomes (PI), (PIE) built high-           dichotomous classifiers. Multiple logistic regression [11] <br /> throughput experimental interaction data sets,          commonly used model application <br />  <br />  <br />                                                                                                                    Page 2 11 <br />                                                                                              (page number citation purposes) <br />  BMC Bioinformatics 2004, 5:154                                                   http://www.biomedcentral.com/1471-2105/5/154 <br />  <br />  <br />  <br /> [12,13]. alternative, sophisticated supervised           Table 1: Order variables enter final model stepwise <br /> learning approach apply random forest algo-       selection logistic regression <br /> rithm [14]. Note  focus            Variables                                                 Order methods used compute estimated <br /> probabilities predicted protein-protein interactions.         Gavin                                                      1 <br />                                                                   MIPS                                                       2 <br /> Logistic regression                                               Rosetta                                                    3 logistic model advantage provides                                                                  4 <br />                                                                   cellcycle                                                  5 <br /> estimated probability pair proteins interact,                                                                   essentiality                                               6 readily available standard statistical packages.    Rosetta*cellcycle                                          7 <br /> paper, logistic regression analysis generated using       cellcycle*essentiality                                     8 <br /> PROC LOGISTIC SAS/STAT software, Version 9              Ho                                                         9 <br /> SAS   evaluate importance           essentiality                                            10 <br /> different genomic features variable selections.          Uetz                                                       11 available schemes, chose stepwise variable selec-         cellcycle                                               12 <br />                                                                   cellcycle*essentiality                                  13 <br /> tion widely used standard packages. Stepwise <br />                                                                   MIPS*essentiality                                          14 <br /> selection greedy search algorithm selects variables     MIPS*Rosetta                                               15 best marginal prediction power given current <br /> model. quantify importance predictor varia- <br /> bles model fitting, use deviance measure <br />  <br /> -2(log L1 - log L0),                                             Table 2: Deviance reduced model final model                                                                  removing corresponding variables L0 likelihood final model given        Variable                                       Deviance <br /> stepwise selection, L1 likelihood reduced <br /> model removing terms involve correspond-                                                      1376.437 <br /> ing predictor variable final model.         MIPS                                            1333.97 <br /> measure considers prediction power variables          essentiality                                    579.988 training sample random test sam-          Rosetta                                         778.493 <br />                                                                   cellcycle                                      1271.461 <br /> ples.  measure biased                                                                   Ho                                              68.718 <br /> dependence training sample.                                Uetz                                            20.513 <br />                                                                   Gavin                                          1839.181 consider, similarly [1], main effects interaction terms genomic features PIP <br /> (indirect evidence protein-protein interactions) PIE (direct experimental protein-protein interaction <br /> measurements) respectively. Table 1 presents terms       Random forest <br /> remained final model orders enter        &quot;random forest&quot; method [14] supervised learning <br /> final model. Table 2 shows deviance measure pre-          algorithm previously successfully applied dictor variables. Gavin data, Gene Ontology              genomic studies. implemented MIPS functional similarity features, cell cycle gene     randomForest package R [15]. random forest expression data important genomic evi-              ensemble classification trees generated dences predicting protein-protein interactions accord-       bootstrap samples original data. known <br /> ing deviance measure, high-       random forests avoid overfitting usually throughput experimental data sets relevant           better classification accuracy classification trees. significant effects included       natural way evaluate importance feature var- <br /> final model.  logistic model restricted        iables random forest algorithm measure linear form provide optimal solution          increase classification error variables prediction problem. objective         permuted. Intuitively, important variables evaluate variable importance according pre-         permuted, produce larger classification errors. <br /> diction accuracy random test samples. fol-        importance score provided random forest lowing, present results using random             accurate estimate classification error  <br /> forest, sophisticated supervised learning                 siders situation random test samples.  <br /> algorithm.                                                       importance score provides objective evalua- <br />                                                                  tion relative merit different genomic features  <br />  <br />                                                                                                                        Page 3 11 <br />                                                                                                  (page number citation purposes) <br />  BMC Bioinformatics 2004, 5:154                                             http://www.biomedcentral.com/1471-2105/5/154 <br />  <br />  <br />  <br /> protein-protein interaction prediction.           sets negligible effects.  different intrinsic tree structure random forest easily takes     result logistic regression, Gavin data set shown account interactions different varia-       important MIPS Gene Ontology func- <br /> bles avoids complications caused missing data      tional similarity features considering situation occurred modeling procedures.                    random test samples. observations motivated                                                                perform thorough information assessment performed random forest analysis growing             genomic evidences considered. compared  <br /> 5,000 trees. Figure 1 shows importance measures         formance different classification methods (random  genomic evidences used random forest algo-          est, logistic regression Bayesian network), rithm. result agrees logistic      evaluated importance different genomic data- <br /> regression MIPS Gene Ontology functional       sets framework best method  method <br /> similarity features important,            lowest classification error). high-throughput experimental data <br />  <br />  <br />  <br />  <br />          MIPS <br />  <br />  <br />             <br />  <br />         Gavin <br />  <br />  <br />      Cellcycle <br />  <br />  <br />         Essen <br />  <br />  <br />       Rosetta <br />  <br />  <br />             Ho <br />  <br />  <br />            Utz <br />  <br />  <br />             Ito <br />  <br />                   0.000               0.002                  0.004              0.006                  0.008 <br />  <br />                                                               Importance <br />  <br /> Figure 1 measure genomic features random forest algorithm <br /> Importance <br /> Importance measure genomic features random forest algorithm horizontal axis presents importance measure vertical axis denotes genomic features. <br />  <br /> Figure <br /> ROC curves <br />        2 random forest, logistic regression Bayesian networks using 7-fold cross validations <br /> ROC curves random forest, logistic regression Bayesian networks using 7-fold cross validations <br />  <br />  <br />  <br />  <br /> Comparison methods                                     MIPS gene ontology functional similarity data conducted 7-fold cross validations subset        saw MIPS Gene Ontology functional sim- <br /> complete information (described  fea-          ilarities important information sources <br /> tures random forest, logistic regression Baye-      logistic regression random forests <br /> sian network method. Figure 2 displays receiver           methods. Histograms MIPS functional sim- <br /> operating characteristic (ROC) curves, observe       ilarity data (Figures 3 4) dif- <br /> better performance random forest          ferent gold standard positives negatives; similar performances logistic regression        protein pairs gold standard positives associated Bayesian network.                                       smaller functional similarity values gold <br />                                                                 standard negatives. pattern explains func- <br /> Information assessment                                          tional similarity features strong impact Information assessment different genomic data            classification accuracy model fitting, observed help understand relationship protein-pro-         Figure 2.  vast number protein pairs tein interactions, form guideline future model        gold standard negatives likely development.                                                    thoroughly studied researchers, henceforth <br />  <br />  <br />   <br /> Figure 3 MIPS Gene Ontology function data gold standard positives negatives <br /> Histograms <br /> Histograms MIPS Gene Ontology function data gold standard positives negatives <br />  <br />  <br />  <br />  observed belong large functional categories              genomic features included, (ii) MIPS Gene <br /> actually divided specific cate-           Ontology functional similarities  (iii) genomic <br /> gories. conjecture suggests information            features MIPS Gene Ontology <br /> MIPS Gene Ontology function data possibly caused               functional similarities. random forest performance selection biases intrinsic biological rele-             evaluated classification error (Err) defined vance. deserves investigations relationship         follows. gold standards MIPS Gene <br /> Ontology functional similarity data.                                  Denote Err1 proportion protein pairs misclassi- <br />                                                                       fied gold standard positives, Err2 counter- following paragraphs, quantitatively              gold standard negatives. define MIPS Gene Ontology functional similarities                classification error average Err1 Err2. dominating information contributors predicting <br /> protein-protein interactions, genomic features                            Err1 + Err2 negligible benefit provide credible pre-               Err =                     . <br />                                                                                            2 <br /> dictions  examine performance random forests using different genomic feature sets: <br />  <br /> Figure 4histograms MIPS Gene Ontology function data gold standard positives negatives lower end <br /> Zoom Zoom histograms MIPS Gene Ontology function data gold standard positives negatives lower end <br />  <br />  <br />  <br />  <br /> Err balanced error rate gold standard positives                  achieved C(X) = f1(X) &gt;f2(X)). formula, negatives. Suppose joint probability density func-                   estimate optimal (minimum) classification error <br /> tions predictor features X f1(X) f2(X)                based estimates f1(X) f2(X). study, <br /> gold standard positives negatives, respectively.                         f1(X) f2(X) estimated empirical density <br /> Denote classifier C(X). classification error                   functions. written                                                                              Table 3 presents optimal classification error using      1                         1 <br /> Err = &#226;&#710;&#171; C( X) = 1] f1( X)dX + &#226;&#710;&#171; C( X) = 0] f2 ( X)dX ,       (1)         MIPS Gene Ontology functional similarity data. Using <br />      2                         2                                             MIPS Gene Ontology functional similarity data sets  indicator function equal 1                     results highly accurate classification opti- <br /> true 0  minimal classification error Errmin                  mal error 0.28%. Table 3 shows effects computed minimizing (1) space X.                     data discretizations originally used easy                                                       Bayesian network method (&quot;grouped&quot;). significant dis- <br />                                                                              crepancy optimal classification errors using                    1                                                         raw data discretized data (&quot;grouped&quot;) suggests                    2&#226;&#710;&#171; <br /> Errmin =              min( f1( X), f2 ( X))dX                                discretization causes loss information. <br />  <br />  <br />                                                                                                                                    Page 7 11 <br />                                                                                                              (page number citation purposes) <br />  BMC Bioinformatics 2004, 5:154                                               http://www.biomedcentral.com/1471-2105/5/154 <br />  <br />  <br />  <br /> Table 3: Optimal classification errors using different      Bayesian method terms classifications  like genomic features                                                 Bayesian method, produces estimated probabilities  Variables                        Optimal Classification Error   proteins interact. dichotomous classifier,                                                                  random forest method outperforms methods <br />  MIPS                                       1.69%                considered efficiently uses information, <br />                                          2.15%                computationally expensive. partic- <br />  MIPS                                     0.28%               ular, importance measure provides objective <br />  MIPS (grouped)                             7.31%                assessment different genomic features predicting <br />  (grouped)                               13.35% <br />                                                                  protein-protein interactions simply considering <br />  MIPS (grouped)                          6.34% <br />                                                                  contributions model fitting. findings moti- <br />                                                                  vation look  sensible data resources <br />                                                                  superior models. <br />  genomic features                                           data discretizations used Baye- estimated classification errors using            sian methods decreased classification performance. genomic features random forest frame-           note genomic features datasets investi- <br /> work. Table 4 shows adding genomic evi-           gated highly processed versions dences complete-information subset provides               datasets derived negligible benefit reduces classification       better ways original data account. <br /> accuracy. <br />                                                                  caveat predictions just  compared ROC curves (Figure 5)           defining groups proteins genomic <br /> random forest method using genomic information,              properties protein complexes MIPS data. MIPS functional similarities,            does necessarily mean really represent <br /> genomic information MIPS  Figure 5             protein complexes.  represent groups shows barely gain considering               proteins properties protein com- <br /> genomic information MIPS available;            plexes. <br /> classifications MIPS functional sim- <br /> ilarity data poor complete-information subset.        analysis looked relative weights Note,  subset interaction data          various features predicting protein-protein interac- strongest expression correlations          tions based previous study [1]. looked necessarily complete-information set considered.             particular subset data complete  expect expression correlations              information able  par- stronger source information               ticular subset information, able context.                                                         functional classification features MIPS <br />                                                                  functional catalog Gene Ontology Conclusions                                                      informative particular machine learning algo- restricted problem posed complete-infor-           rithms, random forests effective mation subset, identified MIPS Gene              Bayesian networks.  mind <br /> Ontology functional similarity datasets dominat-          problem issue incomplete <br /> ing information contributors predicting protein-         information. data sets incomplete information <br /> protein interactions framework proposed             Bayesian approaches maybe effective [1]. Random forests based MIPS informa-            easily handle missing information. care- <br /> tion highly accurate classifications.     ful studies needed determine <br /> particular subset complete information, adding          optimum machine learning method genomic data does little improving predictions.          optimum features presence incomplete infor- <br /> MIPS information,  available              mation. great consider small proportion ~18M protein pairs.                genomic features phylogenetic profiles [16]                                                                  local clustering information [17]. just considered alternative non-Bayesian methods              step direction. logistic regression random forest predicting <br /> protein-protein interactions. existing methods require prior information needed Bayesian <br /> approach, fully utilize raw data dis- <br /> cretization. logistic model performs similarly  <br />  <br />  <br />  <br />  <br />  <br /> Table 4: Classification errors random forest algorithm using different genomic features <br />  <br />                      Variables             Err1 (positives)                    Err2 (negatives)                         Err <br />  <br />                      MIPS             114/2104 = 5.42%                     180/172409 = 0.1%                        2.76% <br />                                      165/2104 = 7.80%                     89/172409 = 0.05%                        3.95% <br />                                    1056/2104 = 78.09%                   313/172409 = 0.20%                       25.20% <br />  <br />  <br />  sensitivity <br /> specificity <br />  <br />  <br /> Figure <br /> ROC  curves <br />        5    random forest using different genomic feature sets <br /> ROC curves random forest using different genomic feature sets   &#226;&#8364;&#8220; genomic information; &apos;MIPS  &#226;&#8364;&#8220; MIPS Gene Ontology function data;   &#226;&#8364;&#8220; genomic features MIPS Gene Ontology function data <br />  <br />  <br />  <br />  <br />  <br />  <br />  <br /> Methods                                                              Additional material <br /> Logistic regression <br /> Denote gold standards random variable Y genomic features X1, X2, ..., Xn. Let Y = 1                                                                           Additional File 1 <br />                                                                           complete-information subset ZIP file. proteins interact, e., complex,                Click file Y = 0  logistic model form                     [http://www.biomedcentral.com/content/supplementary/1471- <br />                                                                           2105-5-154-S1.zip] <br />         Pr(Y = 1) <br /> log                 = &#206;&#177; + &#206;&#178; X, <br />       1 &#226;&#710;&#8217; Pr(Y = 1) random vector X consists X1, X2, ..., Xn             interaction terms.                                         <br /> Stepwise variable selection stepwise selection procedure starts null model.        step, adds variable significant              <br /> score statistics model,               <br /> sequentially removes variable score sta-     <br /> tistic model score statistics            significant. process terminates varia-     <br /> ble added model variable just          <br /> entered model variable removed          <br /> subsequent elimination.  score statistic measures     significance effect variable.            <br /> ROC curve analysis                                                        <br />                                                                         <br /> Receiving operator characteristic (ROC) curve [18]         <br /> graphical representation used assess discriminatory      <br /> ability dichotomous classifier showing                        <br /> tradeoffs sensitivity specificity. Sensitivity cal-      <br /> culated dividing number true positives (TP)        number positives, equals sum              true positives false negatives (FN); specificity   calculated dividing number true negatives                  <br /> (TN) number negatives, equals          sum true negatives false positives (FP).     <br /> Sensitivity = TP/(TP + FN), Specificity = TN/(TN + FP).      ROC curve plot shows 1 - specificity X axis        <br /> sensitivity Y axis. good classifier ROC curve     <br /> climbing rapidly upper left hand corner               <br /> graph. quantified measuring area            curve. closer area 1.0, better classifier  closer area 0.5, worse          <br /> classifier                                                      <br /> Authors&apos; contributions                                                    <br /> NL BW conducted major data analysis, created tables figures, supervi-      <br /> sion HZ. RJ MG provided data sets anal-            <br /> ysis contributed discussion comparisons          different methods. authors read approved                   <br /> final manuscript.                                                      <br />  <br />  <br />  <br /> </body></html>