<?xml version="1.0" encoding="UTF-8"?><html xmlns="http://www.w3.org/1999/xhtml" xmlns:oboInOwl="http://www.geneontology.org/formats/oboInOwl#" xmlns:NCBITaxon="http://purl.obolibrary.org/obo/NCBITaxon#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:obo="http://purl.obolibrary.org/obo/" xmlns:owl="http://www.w3.org/2002/07/owl#" xmlns:ncbitaxon="http://purl.obolibrary.org/obo/ncbitaxon#" xmlns:xsd="http://www.w3.org/2001/XMLSchema#" xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#" xmlns:dublincorens="http://purl.org/dc/elements/1.1/"  version="XHTML+RDFa 1.0" ><body> <br /> Information assessment on predicting protein-protein interactions <br />  <br />                  Abstract <br />                  Background: Identifying protein-protein interactions is fundamental for understanding the <br />                  molecular machinery of the cell. Proteome-wide studies of protein-protein interactions are of <br />                  significant value, but the high-throughput experimental technologies suffer from high rates of both <br />                  false positive and false negative predictions. In addition to high-throughput experimental <span id='am-22' about='obo:NCBITaxon_1369087' typeof='obo:NCBITaxon_37567 owl:Thing obo:NCBITaxon_33340 obo:NCBITaxon_33213 obo:NCBITaxon_1 obo:NCBITaxon_131567 obo:NCBITaxon_85604 obo:NCBITaxon_104431 obo:NCBITaxon_33154 obo:NCBITaxon_37570 obo:NCBITaxon_41191 obo:NCBITaxon_33317 obo:NCBITaxon_2759 obo:NCBITaxon_6072 obo:NCBITaxon_6960 obo:NCBITaxon_33208 obo:NCBITaxon_41196 obo:NCBITaxon_197562 obo:NCBITaxon_85512 obo:NCBITaxon_197563 obo:NCBITaxon_95182 obo:NCBITaxon_41197 obo:NCBITaxon_6656 obo:NCBITaxon_7496 obo:NCBITaxon_7088 obo:NCBITaxon_1206794 obo:NCBITaxon_88770 obo:NCBITaxon_7100 obo:NCBITaxon_33392 obo:NCBITaxon_50557'><span id='am-23' property="oboInOwl:hasOBONamespace" content="ncbi_taxonomy" datatype="xsd:string"></span><span id='am-24' property="rdfs:label" content="Data" datatype="xsd:string"></span><span id='am-25' property="oboInOwl:hasDbXref" content="GC_ID:1" datatype="xsd:string"></span><span id='am-26'  ></span>data</span>, many <br />                  diverse types of genomic data can help predict protein-protein interactions, such as mRNA <br />                  expression, localization, essentiality, and functional annotation. Evaluations of the information <br />                  contributions from different evidences help to establish more parsimonious models with <br />                  comparable or better prediction accuracy, and to obtain biological insights of the relationships <br />                  between protein-protein interactions and other genomic information. <br />                  Results: Our assessment is based on the genomic features used in a Bayesian network approach <br />                  to predict protein-protein interactions genome-wide in yeast. In the special case, when one does <br />                  not have any missing information about any of the features, our analysis shows that there is a larger <br />                  information contribution from the functional-classification than from expression correlations or <br />                  essentiality. We also show that in <span id='am-9' about='obo:NCBITaxon_169495' typeof='owl:Thing obo:NCBITaxon_33340 obo:NCBITaxon_33213 obo:NCBITaxon_169440 obo:NCBITaxon_1 obo:NCBITaxon_131567 obo:NCBITaxon_7147 obo:NCBITaxon_43733 obo:NCBITaxon_33154 obo:NCBITaxon_43738 obo:NCBITaxon_480117 obo:NCBITaxon_33317 obo:NCBITaxon_169449 obo:NCBITaxon_480118 obo:NCBITaxon_2759 obo:NCBITaxon_6072 obo:NCBITaxon_6960 obo:NCBITaxon_43750 obo:NCBITaxon_33208 obo:NCBITaxon_85512 obo:NCBITaxon_197562 obo:NCBITaxon_197563 obo:NCBITaxon_7203 obo:NCBITaxon_6656 obo:NCBITaxon_7496 obo:NCBITaxon_1206794 obo:NCBITaxon_88770 obo:NCBITaxon_33392 obo:NCBITaxon_169455 obo:NCBITaxon_50557 obo:NCBITaxon_43741'><span id='am-10' property="rdfs:label" content="This" datatype="xsd:string"></span><span id='am-11'  ></span><span id='am-12' property="oboInOwl:hasOBONamespace" content="ncbi_taxonomy" datatype="xsd:string"></span><span id='am-13' property="oboInOwl:hasDbXref" content="GC_ID:1" datatype="xsd:string"></span>this</span> case alternative models, such as logistic regression and <br />                  random forest, may be more effective than Bayesian networks for predicting interactions. <br />                  Conclusions: In the restricted problem posed by the complete-information subset, we identified <br />                  that the MIPS and Gene Ontology (GO) functional similarity datasets as the dominating information <br />                  contributors for predicting the protein-protein interactions under the framework proposed by <br />                  Jansen et al. Random forests based on the MIPS and GO information alone can give highly accurate <br />                  classifications. In this particular subset of complete information, adding other genomic data does <br />                  little for improving predictions. We also found that the data discretizations used in the Bayesian <br />                  methods decreased classification performance. <br />  <br />  <br />  <br />  <br /> Background                                                                     catalyze large numbers of chemical reactions, and are cru- <br /> Proteins transmit regulatory signals throughout the cell,                      cial for the stability of numerous cellular structures. <br />  <br />                                                                                                                                         Page 1 of 11 <br />                                                                                                                   (page number not for citation purposes) <br />  BMC Bioinformatics 2004, 5:154                                              http://www.biomedcentral.com/1471-2105/5/154 <br />  <br />  <br />  <br /> Interactions among proteins are key for cell functioning        other (PIP) on the mRNA expression, Gene Ontology, <br /> and identifying such interactions is crucial for deciphering    MIPS functional and coessentiality data. Within the PIP <br /> the fundamental molecular mechanisms of the cell. As rel-       sub-network, different genomic features are assumed to <br /> evant genomic information is exponentially increasing           be independent in prior. In addition, this method <br /> both in quantity and complexity, in silico predictions of       involved discretizing the raw data into groups and repre- <br /> protein-protein interactions have been possible but also        senting the two mRNA expression profiles (cell cycle and <br /> challenging. A number of techniques have been devel-            Rosetta compendium data) by their first principal compo- <br /> oped that exploit combinations of protein features in           nent for computational convenience. <br /> training data and can predict protein-protein interactions <br /> when applied to novel proteins. Our study is motivated by       Our current study focuses on assessing the contributions <br /> a study by Jansen et al. [1], who proposed a Bayesian           of different types of genomic data towards predicting pro- <br /> method to use the MIPS [2] complexes catalog as gold            tein-protein interactions. This may help us to understand <br /> standard positives and lists of proteins in separate subcel-    which genomic features have the closest biological rela- <br /> lular compartments [3] as gold standard negatives. The          tionship with protein-protein interactions and hence to <br /> various protein features considered in this method              construct a better prediction model. As prediction rules <br /> include time course mRNA expression fluctuations during         involving less relevant information may have lower pre- <br /> the yeast cell cycle [4] and the Rosetta compendium [5],        diction accuracy, our analysis can give us insights into <br /> biological function data from the Gene Ontology [6] and         how to construct more parsimonious models with com- <br /> the MIPS functional catalog, essentiality data [2], and         parable or better prediction accuracy. A potential disad- <br /> high-throughput experimental interaction data [7-10].           vantage of the Bayesian network approach may be that the <br /> The MIPS and Gene Ontology functional annotations are           data discretization can obscure information contained in <br /> used for quantifying the functional similarity between          the raw genomic data. Thus, in addition to assessing the <br /> two proteins. The MIPS functional catalog (or GO biolog-        information content of the data sources, we also propose <br /> ical process annotation) can be thought of as a hierarchi-      alternative non-Bayesian models that fully utilize the data <br /> cal tree of functional classes (or a directed acyclic graph     without discretization. These methods, such as logistic <br /> (DAG) in the case of GO). Each protein is either a member       regression and random forests, do not require prior <br /> or not a member of each functional <span id='am-6' about='obo:NCBITaxon_class' typeof='owl:Thing NCBITaxon:_taxonomic_rank'><span id='am-7' property="rdfs:label" content="class" datatype="xsd:string"></span><span id='am-8' property="oboInOwl:hasOBONamespace" content="ncbi_taxonomy" datatype="xsd:string"></span>class</span>, such that each        knowledge, and we can evaluate the importance of the dif- <br /> protein describes a &quot;subtree&quot; of the overall hierarchical       ferent genomic features in the context of these methods. <br /> tree of classes (or subgraph of the DAG in the case of GO). <br /> Given two proteins, one can compute the intersection tree       Results and discussions <br /> of the two subtrees associated with these proteins. This        To accurately and quantitatively assess the information <br /> intersection tree can be computed for the complete list of      contributions of different genomic features, we construct <br /> protein pairs (where both proteins of each pair are in the      in essence a simplified problem that has some but not all <br /> functional classification), and thus a distribution of inter-   of the elements of the original study. Here, we only look <br /> section trees is obtained. Then the &quot;functional similarity&quot;     at a subset of the data from [1] comprising the 18 million <br /> between two proteins is defined as the frequency at which       protein pairs in total and approximately 8,000 gold stand- <br /> the intersection tree of the two proteins occurs in the dis-    ard positives and 2.7 million gold standard negatives. This <br /> tribution. Intuitively, the intersection tree gives the func-   subset (see Additional File 1) contains 2,104 positives and <br /> tional annotation that two proteins share. The more             172,409 negatives. In this subset, we have complete infor- <br /> ubiquitous this shared functional annotation is, the larger     mation for each feature and we can thus quantitatively <br /> is the functional similarity frequency; the more specific       assess the relative contributions of the different features <br /> the shared functional annotation is, the smaller is the         on this set. This data set can be downloaded from http:// <br /> functional similarity frequency. The essentiality data rep-     bioinformatics.med.yale.edu/PPI. In doing so, we find <br /> resents a categorical variable that denotes whether zero,       that some of the features have stronger influence on the <br /> one or both proteins in a protein pair are essential. The       overall prediction. While this might be true for the larger <br /> supplementary online material of [1]http://www.science          problem as well, there are a number of caveats that one <br /> mag.org/cgi/data/302/5644/449/DC1/1 provides more               has to keep in mind, such as that the features that are <br /> details about the quantification of these variables. Their      present in this subset might not be the strongest in the <br /> Bayesian method predicts protein-protein interactions           whole set of 18 million protein pairs. <br /> genome-wide by probabilistic integration of genomic fea- <br /> tures that are weakly associated with interactions (mRNA        Alternative models <br /> expression, essentiality and localization). The model was       Here, we construct models for predicting protein-protein <br /> used for two separate predictions of probabilistic interac-     interactions that, given the gold standards, are basically <br /> tomes (PI), one of which (PIE) is built on four high-           dichotomous classifiers. Multiple logistic regression [11] <br /> throughput experimental interaction data sets, and the          is one commonly used model for such an application <br />  <br />  <br />                                                                                                                    Page 2 of 11 <br />                                                                                              (page number not for citation purposes) <br />  BMC Bioinformatics 2004, 5:154                                                   http://www.biomedcentral.com/1471-2105/5/154 <br />  <br />  <br />  <br /> [12,13]. An alternative, more sophisticated supervised           Table 1: <span id='am-19' about='obo:NCBITaxon_order' typeof='owl:Thing NCBITaxon:_taxonomic_rank'><span id='am-20' property="oboInOwl:hasOBONamespace" content="ncbi_taxonomy" datatype="xsd:string"></span><span id='am-21' property="rdfs:label" content="order" datatype="xsd:string"></span>Order</span> of variables that enter the final model by stepwise <br /> learning approach that we apply is the random forest algo-       selection in logistic regression <br /> rithm [14]. Note that, although not our focus here, all           Variables                                                 Order <br /> these methods can be used to compute the estimated <br /> probabilities for predicted protein-protein interactions.         Gavin                                                      1 <br />                                                                   MIPS                                                       2 <br /> Logistic regression                                               Rosetta                                                    3 <br /> The logistic model has the advantage that it provides an          GO                                                         4 <br />                                                                   cellcycle                                                  5 <br /> estimated probability that a pair of proteins interact, and <br />                                                                   essentiality                                               6 <br /> is readily available in standard statistical packages. In this    Rosetta*cellcycle                                          7 <br /> paper, the logistic regression analysis was generated using       cellcycle*essentiality                                     8 <br /> PROC LOGISTIC in SAS/STAT software, Version 9 of the              Ho                                                         9 <br /> SAS System. Moreover, we can evaluate the importance of           GO*essentiality                                            10 <br /> different genomic features by variable selections. Among          Uetz                                                       11 <br /> many available schemes, we chose stepwise variable selec-         GO*cellcycle                                               12 <br />                                                                   GO*cellcycle*essentiality                                  13 <br /> tion that is widely used in standard packages. Stepwise <br />                                                                   MIPS*essentiality                                          14 <br /> selection is a greedy search algorithm that selects variables     MIPS*Rosetta                                               15 <br /> with the best marginal prediction power given the current <br /> model. To quantify the importance of the predictor varia- <br /> bles to the model fitting, we can use the deviance measure <br />  <br /> -2(log L1 - log L0),                                             Table 2: Deviance of the reduced model from the final model by <br />                                                                  removing corresponding variables <br /> where L0 is the likelihood of the final model given by the        Variable                                       Deviance <br /> stepwise selection, and L1 is the likelihood of the reduced <br /> model by removing all terms that involve the correspond-          GO                                             1376.437 <br /> ing predictor variable from the final model. However, this        MIPS                                            1333.97 <br /> measure only considers the prediction power of variables          essentiality                                    579.988 <br /> for the training sample but not for any random test sam-          Rosetta                                         778.493 <br />                                                                   cellcycle                                      1271.461 <br /> ples. Therefore, this measure can be biased due to its <br />                                                                   Ho                                              68.718 <br /> dependence on the training sample.                                Uetz                                            20.513 <br />                                                                   Gavin                                          1839.181 <br /> We consider, similarly as in [1], all the main effects and <br /> interaction terms among the genomic features in the PIP <br /> (indirect evidence for protein-protein interactions) and <br /> the PIE (direct experimental protein-protein interaction <br /> measurements) respectively. Table 1 presents all the terms       Random forest <br /> remained in the final model and their orders to enter the        The &quot;random forest&quot; method [14] is a supervised learning <br /> final model. Table 2 shows the deviance measure of pre-          algorithm that has previously been successfully applied to <br /> dictor variables. The Gavin data, Gene Ontology and              many genomic studies. It has been implemented in the <br /> MIPS functional similarity features, and the cell cycle gene     randomForest package of R [15]. A random forest is an <br /> expression data are the most important genomic evi-              ensemble of many classification trees generated from <br /> dences for predicting protein-protein interactions accord-       bootstrap samples of the original data. It is well known <br /> ing to the deviance measure, whereas the three other high-       that random forests avoid overfitting and usually have <br /> throughput experimental data sets are less relevant or           better classification accuracy than classification trees. A <br /> even do not have significant effects to be included in the       natural way to evaluate the importance of the feature var- <br /> final model. However, the logistic model is restricted by        iables with the random forest algorithm is to measure the <br /> its linear form and may not provide an optimal solution          increase of the classification error when those variables <br /> to the prediction problem. And it will be more objective         are permuted. Intuitively, the more important variables <br /> to evaluate the variable importance according to its pre-        will, when permuted, produce larger classification errors. <br /> diction accuracy for any random test samples. In the fol-        The importance score provided by the random forest is a <br /> lowing, we present the results from using the random             more accurate estimate of the classification error that con- <br /> forest, a more sophisticated supervised learning                 siders the situation of random test samples. Therefore, <br /> algorithm.                                                       this importance score provides a more objective evalua- <br />                                                                  tion of the relative merit of different genomic features on <br />  <br />  <br />                                                                                                                        Page 3 of 11 <br />                                                                                                  (page number not for citation purposes) <br />  BMC Bioinformatics 2004, 5:154                                             http://www.biomedcentral.com/1471-2105/5/154 <br />  <br />  <br />  <br /> protein-protein interaction prediction. Moreover, the          sets have negligible effects. However, different from the <br /> intrinsic tree structure of the random forest easily takes     result from logistic regression, the Gavin data set is shown <br /> into account the interactions among the different varia-       to be less important than MIPS and Gene Ontology func- <br /> bles and avoids complications caused by missing data that      tional similarity features after considering the situation of <br /> occurred in many other modeling procedures.                    random test samples. These observations motivated us to <br />                                                                perform a more thorough information assessment of the <br /> We performed our random forest analysis by growing             genomic evidences considered. We first compared the per- <br /> 5,000 trees. Figure 1 shows the importance measures of         formance of different classification methods (random for- <br /> the genomic evidences used in the random forest algo-          est, logistic regression and Bayesian network), and then <br /> rithm. The result agrees mostly with that of the logistic      evaluated the importance of the different genomic data- <br /> regression in that the MIPS and Gene Ontology functional       sets within the framework the best method (the method <br /> similarity features are found to be very important,            with the lowest classification error). <br /> whereas most of the high-throughput experimental data <br />  <br />  <br />  <br />  <br />          MIPS <br />  <br />  <br />            GO <br />  <br />  <br />         Gavin <br />  <br />  <br />      Cellcycle <br />  <br />  <br />         Essen <br />  <br />  <br />       Rosetta <br />  <br />  <br />             Ho <br />  <br />  <br />            Utz <br />  <br />  <br />             Ito <br />  <br />                   0.000               0.002                  0.004              0.006                  0.008 <br />  <br />                                                               Importance <br />  <br /> Figure 1 measure of genomic features from the random forest algorithm <br /> Importance <br /> Importance measure of genomic features from the random forest algorithm The horizontal <span id='am-14' about='obo:NCBITaxon_9855' typeof='owl:Thing obo:NCBITaxon_9850 obo:NCBITaxon_33213 obo:NCBITaxon_1 obo:NCBITaxon_91561 obo:NCBITaxon_35500 obo:NCBITaxon_32523 obo:NCBITaxon_131567 obo:NCBITaxon_32524 obo:NCBITaxon_117570 obo:NCBITaxon_32525 obo:NCBITaxon_89593 obo:NCBITaxon_7742 obo:NCBITaxon_117571 obo:NCBITaxon_33154 obo:NCBITaxon_1437010 obo:NCBITaxon_34878 obo:NCBITaxon_1338369 obo:NCBITaxon_40674 obo:NCBITaxon_8287 obo:NCBITaxon_2759 obo:NCBITaxon_314145 obo:NCBITaxon_6072 obo:NCBITaxon_33511 obo:NCBITaxon_33208 obo:NCBITaxon_7711 obo:NCBITaxon_7776 obo:NCBITaxon_9347 obo:NCBITaxon_9845'><span id='am-15' property="oboInOwl:hasDbXref" content="GC_ID:1" datatype="xsd:string"></span><span id='am-16' property="rdfs:label" content="Axis" datatype="xsd:string"></span><span id='am-17'  ></span><span id='am-18' property="oboInOwl:hasOBONamespace" content="ncbi_taxonomy" datatype="xsd:string"></span>axis</span> presents the <br /> importance measure whereas the vertical axis denotes the genomic features. <br />  <br /> Figure <br /> ROC curves <br />        2 of random forest, logistic regression and Bayesian networks using 7-fold cross validations <br /> ROC curves of random forest, logistic regression and Bayesian networks using 7-fold cross validations <br />  <br />  <br />  <br />  <br /> Comparison of three methods                                     MIPS and gene ontology functional similarity data <br /> We conducted 7-fold cross validations on the subset with        We saw that the MIPS and Gene Ontology functional sim- <br /> complete information (described above) on all the fea-          ilarities were the two most important information sources <br /> tures for random forest, logistic regression and the Baye-      under both the logistic regression and random forests <br /> sian network method. Figure 2 displays their receiver           methods. Histograms of the MIPS and GO functional sim- <br /> operating characteristic (ROC) curves, where we observe a       ilarity data (Figures 3 and 4) show that they are very dif- <br /> better performance of the random forest over the other          ferent for the gold standard positives and negatives; <br /> two and similar performances between logistic regression        protein pairs in the gold standard positives are associated <br /> and the Bayesian network.                                       with smaller functional similarity values than the gold <br />                                                                 standard negatives. This pattern explains why the func- <br /> Information assessment                                          tional similarity features have such a strong impact on <br /> Information assessment of different genomic data may            classification accuracy in the model fitting, as observed in <br /> help us understand their relationship with protein-pro-         Figure 2. However, the vast number of protein pairs in the <br /> tein interactions, and form a guideline for future model        gold standard negatives are likely to be those that have not <br /> development.                                                    been thoroughly studied by researchers, and henceforth <br />  <br />  <br />   <br /> Figure 3 of MIPS and Gene Ontology function data for gold standard positives and negatives <br /> Histograms <br /> Histograms of MIPS and Gene Ontology function data for gold standard positives and negatives <br />  <br />  <br />  <br />  <br /> are observed to belong to large functional categories that            (i) all genomic features included, (ii) MIPS and Gene <br /> actually should be further divided into more specific cate-           Ontology functional similarities only, and (iii) genomic <br /> gories. This conjecture suggests that the information from            features other than the MIPS and Gene Ontology <br /> MIPS and Gene Ontology function data is possibly caused               functional similarities. The random forest performance is <br /> by selection biases other than intrinsic biological rele-             evaluated with the classification error (Err) defined as <br /> vance. It deserves further investigations of the relationship         follows. <br /> between the gold standards and the MIPS and Gene <br /> Ontology functional similarity data.                                  Denote Err1 as the proportion of protein pairs misclassi- <br />                                                                       fied in the gold standard positives, and Err2 the counter- <br /> In the following paragraphs, we show quantitatively that              part for the gold standard negatives. Then we define the <br /> the MIPS and Gene Ontology functional similarities are                classification error as the average of Err1 and Err2. <br /> the dominating information contributors for predicting <br /> protein-protein interactions, while other genomic features                            Err1 + Err2 <br /> have negligible benefit and can not provide credible pre-               Err =                     . <br />                                                                                            2 <br /> dictions by themselves. We examine the performance of <br /> random forests using three different genomic feature sets: <br />  <br /> Figure 4histograms of MIPS and Gene Ontology function data for gold standard positives and negatives on the lower end <br /> Zoom-in <br /> Zoom-in histograms of MIPS and Gene Ontology function data for gold standard positives and negatives on the lower end <br />  <br />  <br />  <br />  <br /> Err is a balanced error rate across gold standard positives                  is achieved at C(X) = I(f1(X) &gt;f2(X)). With this formula, we <br /> and negatives. Suppose the joint probability density func-                   can estimate the optimal (minimum) classification error <br /> tions of the predictor features X are f1(X) and f2(X) for the                based on any estimates of f1(X) and f2(X). In our study, <br /> gold standard positives and negatives, respectively.                         f1(X) and f2(X) are estimated by their empirical density <br /> Denote a classifier by C(X). Then the classification error                   functions. <br /> can be written as <br />                                                                              Table 3 presents the optimal classification error using the <br />      1                         1 <br /> Err = &#8747; I[C( X) = 1] f1( X)dX + &#8747; I[C( X) = 0] f2 ( X)dX ,       (1)         MIPS and Gene Ontology functional similarity data. Using <br />      2                         2                                             the MIPS and Gene Ontology functional similarity data sets <br /> where I(A) is an <span id='am-1' about='obo:NCBITaxon_189528' typeof='owl:Thing obo:NCBITaxon_33213 obo:NCBITaxon_1329799 obo:NCBITaxon_1 obo:NCBITaxon_32523 obo:NCBITaxon_131567 obo:NCBITaxon_32524 obo:NCBITaxon_436489 obo:NCBITaxon_117570 obo:NCBITaxon_89593 obo:NCBITaxon_7742 obo:NCBITaxon_117571 obo:NCBITaxon_33154 obo:NCBITaxon_8782 obo:NCBITaxon_436486 obo:NCBITaxon_1338369 obo:NCBITaxon_8287 obo:NCBITaxon_2759 obo:NCBITaxon_8492 obo:NCBITaxon_6072 obo:NCBITaxon_189527 obo:NCBITaxon_33511 obo:NCBITaxon_33208 obo:NCBITaxon_7711 obo:NCBITaxon_8825 obo:NCBITaxon_7776 obo:NCBITaxon_436492 obo:NCBITaxon_32561 obo:NCBITaxon_8457 obo:NCBITaxon_9219 obo:NCBITaxon_436491'><span id='am-2'  ></span><span id='am-3' property="oboInOwl:hasOBONamespace" content="ncbi_taxonomy" datatype="xsd:string"></span><span id='am-4' property="oboInOwl:hasDbXref" content="GC_ID:1" datatype="xsd:string"></span><span id='am-5' property="rdfs:label" content="Indicator" datatype="xsd:string"></span>indicator</span> function equal to 1 when A is                     alone results in a highly accurate classification with an opti- <br /> true and 0 otherwise. A minimal classification error Errmin                  mal error of only 0.28%. Table 3 also shows the effects of <br /> can be computed by minimizing (1) across the space of X.                     the data discretizations that were originally used in the <br /> It is easy to see that                                                       Bayesian network method (&quot;grouped&quot;). The significant dis- <br />                                                                              crepancy between optimal classification errors using the <br />                    1                                                         raw data and the discretized data (&quot;grouped&quot;) suggests that <br />                    2&#8747; <br /> Errmin =              min( f1( X), f2 ( X))dX                                the discretization causes serious loss of information. <br />  <br />  <br />                                                                                                                                    Page 7 of 11 <br />                                                                                                              (page number not for citation purposes) <br />  BMC Bioinformatics 2004, 5:154                                               http://www.biomedcentral.com/1471-2105/5/154 <br />  <br />  <br />  <br /> Table 3: Optimal classification errors when using different      Bayesian method in terms of classifications and, like the <br /> genomic features                                                 Bayesian method, produces estimated probabilities that <br />  Variables                        Optimal Classification Error   two proteins interact. As a dichotomous classifier, the <br />                                                                  random forest method outperforms the other methods <br />  MIPS                                       1.69%                considered and efficiently uses the information, <br />  GO                                         2.15%                although it is computationally more expensive. In partic- <br />  MIPS+GO                                     0.28%               ular, its importance measure provides a more objective <br />  MIPS (grouped)                             7.31%                assessment of different genomic features on predicting <br />  GO (grouped)                               13.35% <br />                                                                  protein-protein interactions than simply considering <br />  MIPS+GO (grouped)                          6.34% <br />                                                                  contributions to model fitting. These findings are moti- <br />                                                                  vation to look for other, more sensible data resources <br />                                                                  and superior models. <br />  <br /> Other genomic features                                           We found that the data discretizations used in the Baye- <br /> We also estimated the classification errors using the            sian methods decreased classification performance. We <br /> other genomic features within the random forest frame-           note here that the genomic features datasets investi- <br /> work. Table 4 shows that adding the other genomic evi-           gated here themselves are highly processed versions of <br /> dences in the complete-information subset provides               the datasets they were derived from and that there may <br /> only negligible benefit or even reduces the classification       be better ways to take the original data into account. <br /> accuracy. <br />                                                                  Another caveat is that the predictions might be just <br /> Moreover, we compared the ROC curves (Figure 5) of the           defining groups of proteins that have the same genomic <br /> random forest method using all genomic information,              properties as the protein complexes in the MIPS data. <br /> only the MIPS and GO functional similarities, and the            This does not necessarily mean that they really represent <br /> genomic information other than MIPS and GO. Figure 5             protein complexes. Rather, they may represent groups of <br /> shows that we barely gain any by considering other               proteins that have the same properties as protein com- <br /> genomic information if the MIPS and GO are available;            plexes. <br /> classifications without the MIPS and GO functional sim- <br /> ilarity data are poor on the complete-information subset.        In this analysis we have looked at the relative weights of <br /> Note, however, that the subset of full interaction data          various features in predicting protein-protein interac- <br /> which have the strongest expression correlations is not          tions based on the previous study in [1]. We looked at a <br /> necessarily the complete-information set considered.             particular subset of the data where we had complete <br /> Hence, we would expect that expression correlations              information and we were able to show that, for this par- <br /> might be a stronger source of information in other               ticular subset of the full information, we are able to show <br /> context.                                                         that the functional classification features in the MIPS <br />                                                                  functional catalog and Gene Ontology were the most <br /> Conclusions                                                      informative and that particular machine learning algo- <br /> In the restricted problem posed by the complete-infor-           rithms, such as random forests were more effective than <br /> mation subset, we identified that the MIPS and Gene              Bayesian networks. However, one has to keep in mind <br /> Ontology functional similarity datasets as the dominat-          that in the full problem there is the issue of incomplete <br /> ing information contributors for predicting the protein-         information. On data sets with incomplete information <br /> protein interactions under the framework proposed in             Bayesian approaches maybe more effective because they <br /> [1]. Random forests based on the MIPS and GO informa-            can easily handle the missing information. Further care- <br /> tion alone can give highly accurate classifications. In this     ful studies such as these will be needed to determine <br /> particular subset of complete information, adding other          what the optimum machine learning method is and the <br /> genomic data does little for improving predictions. The          optimum features are in presence of incomplete infor- <br /> MIPS and GO information, however, is only available              mation. It will be also of great interest to consider other <br /> for a small proportion of the ~18M protein pairs.                genomic features such as phylogenetic profiles [16] and <br />                                                                  local clustering information [17]. This is just the first <br /> We considered alternative non-Bayesian methods such              step in that direction. <br /> as logistic regression and random forest for predicting <br /> protein-protein interactions. These existing methods do <br /> not require prior information needed for the Bayesian <br /> approach, and can fully utilize the raw data without dis- <br /> cretization. The logistic model performs similarly as the <br />  <br />  <br />  <br />  <br />  <br />  <br /> Table 4: Classification errors of the random forest algorithm when using different genomic features <br />  <br />                      Variables             Err1 (positives)                    Err2 (negatives)                         Err <br />  <br />                      MIPS+GO             114/2104 = 5.42%                     180/172409 = 0.1%                        2.76% <br />                        ALL               165/2104 = 7.80%                     89/172409 = 0.05%                        3.95% <br />                        ELSE             1056/2104 = 78.09%                   313/172409 = 0.20%                       25.20% <br />  <br />  <br />  sensitivity <br /> specificity <br />  <br />  <br /> Figure <br /> ROC  curves <br />        5    of random forest using different genomic feature sets <br /> ROC curves of random forest using different genomic feature sets &apos;All&apos; &#8211; all genomic information; &apos;MIPS+GO&apos; &#8211; only <br /> MIPS and Gene Ontology function data; &apos;ELSE&apos; &#8211; genomic features other than MIPS and Gene Ontology function data <br />  <br />  <br />  <br />  <br />  <br />  <br />  <br /> Methods                                                              Additional material <br /> Logistic regression <br /> Denote the gold standards by random variable Y and the <br /> other genomic features by X1, X2, ..., Xn. Let Y = 1 when <br />                                                                           Additional File 1 <br />                                                                           The complete-information subset in ZIP file. <br /> two proteins interact, i.e., they are in the same complex,                Click here for file <br /> and Y = 0 when not. The logistic model is of the form                     [http://www.biomedcentral.com/content/supplementary/1471- <br />                                                                           2105-5-154-S1.zip] <br />         Pr(Y = 1) <br /> log                 = &#945; + &#946; X, <br />       1 &#8722; Pr(Y = 1) <br /> where the random vector X consists of X1, X2, ..., Xn and             <br /> their interaction terms.                                         <br /> Stepwise variable selection <br /> The stepwise selection procedure starts from a null model.        <br /> At each step, it adds a variable with the most significant              <br /> score statistics among those not in the model, then               <br /> sequentially removes the variable with the least score sta-     <br /> tistic among those in the model whose score statistics are            <br /> not significant. The process terminates if no further varia-     <br /> ble can be added to the model or if the variable just          <br /> entered into the model is the only variable removed in the          <br /> subsequent elimination. Here, the score statistic measures     <br /> the significance of the effect of a variable.            <br /> ROC curve analysis                                                        <br />                                                                         <br /> Receiving operator characteristic (ROC) curve [18] is a         <br /> graphical representation used to assess the discriminatory      <br /> ability of a dichotomous classifier by showing the                        <br /> tradeoffs between sensitivity and specificity. Sensitivity is cal-      <br /> culated by dividing the number of true positives (TP)        <br /> through the number of all positives, which equals the sum              <br /> of the true positives and the false negatives (FN); specificity   <br /> is calculated by dividing the number of true negatives                  <br /> (TN) through the number of all negatives, which equals          <br /> the sum of the true negatives and the false positives (FP).     <br /> Sensitivity = TP/(TP + FN), Specificity = TN/(TN + FP).      <br /> The ROC curve plot shows 1 - specificity on the X axis and        <br /> sensitivity on the Y axis. A good classifier has its ROC curve     <br /> climbing rapidly towards upper left hand corner of the               <br /> graph. This can also be quantified by measuring the area            <br /> under the curve. The closer the area is to 1.0, the better the <br /> classifier is; and the closer the area is to 0.5, the worse the          <br /> classifier is.                                                     <br /> Authors&apos; contributions                                                    <br /> NL and BW conducted the major part of the data analysis, <br /> and created all the tables and figures, under the supervi-      <br /> sion of HZ. RJ and MG provided the data sets for the anal-            <br /> ysis and contributed to the discussion on the comparisons          <br /> of different methods. All authors read and approved the                   <br /> final manuscript.                                                      <br />  <br />  <br />  <br /> </body></html>